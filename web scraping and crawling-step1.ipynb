{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "check whether the give ur is found or not in the server\n"
      ],
      "metadata": {
        "id": "9SgYWnwat7HV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgkT_JictfN1"
      },
      "outputs": [],
      "source": [
        "from urllib.request import  urlopen #in urllib package there are some modules to get requests,error msg, robot.txt, pass urls\n",
        "from urllib.error import  HTTPError\n",
        "from urllib.error import  URLError\n",
        "\n",
        "try:\n",
        "  html = urlopen(\"https://abcxyz.com\")\n",
        "except HTTPError as e:\n",
        "  print(\"HTTP Error\")\n",
        "except URLError as u:\n",
        "  print(\"URL ERROR :( Server Not Found!\")\n",
        "else:\n",
        "  print(html.read())\n",
        "\n",
        "\n",
        "try:\n",
        "  html = urlopen(\"https://www.sliit.lk/\")\n",
        "except HTTPError as e:\n",
        "  print(\"HTTP Error\")\n",
        "except URLError as u:\n",
        "  print(\"Server Not Found!\")\n",
        "else:\n",
        "  print(\"NO error with the url :) HTML Details\")\n",
        "  print(html.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**output acording to 1st try catch**\n",
        "\n",
        "Server Not Found!-->url error--> bcz there is no url called that name\n",
        "\n",
        "**output acording to 2nd try catch**\n",
        "\n",
        "HTML Details"
      ],
      "metadata": {
        "id": "bFR-pymrvrmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElWUnJrZxq02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#another way to gaet the same using dirrerent module"
      ],
      "metadata": {
        "id": "y8ivYn0txq4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import  urlopen\n",
        "from urllib.error import *\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5aYJi0-vsZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#create function\n",
        "#pass the url\n",
        "def url_ok(url):\n",
        "  #exception block\n",
        "  try:\n",
        "\n",
        "    #pass the url into\n",
        "    #request.hear\n",
        "    response = requests.head(url)\n",
        "\n",
        "    #check the status code\n",
        "    if response.status_code == 200:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  except requests.ConnectionError as e:\n",
        "    return e\n",
        "\n",
        "#driven code\n",
        "url = \"https://www.geeksforgeeks.org/\"\n",
        "url_ok(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2VLGFdOxuTy",
        "outputId": "921fed37-ace2-402e-b658-95ac31ae9c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXxvNJuUvbHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.get(\"https://www.sliit.lk/\") #request.get methode to get the source code can be in robot.txt or html form\n",
        "test = response.text\n",
        "print(\"robot.txt for https://www.sliit.lk/\")\n",
        "print(\"==============================\")\n",
        "print(test)\n",
        "#source code /inspect will be give  as output"
      ],
      "metadata": {
        "id": "2n_CKY4NpSMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "#response = requests.get(\"https://www.sliit.lk/\") #request.get methode to get the source code can be in robot.txt or html form\n",
        "response = requests.get(\"https://www.sliit.lk/robots.txt\")\n",
        "test = response.text\n",
        "print(\"robot.txt for https://www.sliit.lk/\")\n",
        "print(\"==============================\")\n",
        "print(test)\n",
        "#source code /inspect will be give  as output"
      ],
      "metadata": {
        "id": "_fGO6m9BxuWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if you want to scrap some data from source code--> beautiful soup module"
      ],
      "metadata": {
        "id": "VTozs9FPxuZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how to exract the header tag of this wekipedia"
      ],
      "metadata": {
        "id": "2xG_3YBppZFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
        "\n",
        "#you can use\n",
        " #BeautifulSoup --> library to scrap the data(can extract deferent diferent parts in source code)\n",
        " #Scrapy\n",
        "#to scrap the data from the source codes\n",
        "bs = BeautifulSoup(html,\"html.parser\") # #to scrap data from the sourse code\n",
        "\n",
        "#while we are crawl we have to parse the data we can parse either html or xml\n",
        "#html.parser === is the library to handdle html only(read only html)\n",
        "#lxml === is the library to handle bothe xml and html\n",
        "#bs = BeautifulSoup(html) --- if you didnot put anything here ---by default--> html\n",
        "\n",
        "\n",
        "titles = bs.find_all(['h1'])\n",
        "#there are two ways in bs\n",
        " #find\n",
        " #find_all\n",
        "\n",
        "\n",
        "print('List the first header tag :', *titles, sep ='\\n\\n')"
      ],
      "metadata": {
        "id": "6miJiptqpZIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "find_all\n",
        "\n",
        "*** all the header tags from en.wikipedia.org/wiki/Main_Page. ***"
      ],
      "metadata": {
        "id": "GIpbp3ojtx35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
        "\n",
        "bs = BeautifulSoup(html,\"html.parser\") #===bs = BeautifulSoup(html)\n",
        "titles = bs.find_all(['h1', 'h2','h3'])\n",
        "print('List the first header tag :', *titles, sep ='\\n\\n')\n",
        "print('===================================================')\n",
        "print('List the first header tag :', titles, sep ='\\n\\n')  # * --- without astric getting as a [list]"
      ],
      "metadata": {
        "id": "-KDyUcVqtP9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "find - only getting 1 h13"
      ],
      "metadata": {
        "id": "f88skPWHtvqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
        "\n",
        "bs = BeautifulSoup(html,\"html.parser\") #===bs = BeautifulSoup(html)\n",
        "titles = bs.find(['h1', 'h2','h3'])\n",
        "print('List the first header tag :', *titles, sep ='\\n\\n') #  * to extract separatly without list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxGYzP38pZK-",
        "outputId": "6f45de0f-f5a1-422e-8fd4-5ef55106eb5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List the first header tag :\n",
            "\n",
            "<span class=\"mw-page-title-main\">Main Page</span>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqbb2vVSwX-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in web scrappying we cannot use exact static page it can be different with the sites\n",
        "#bcz srapping will be done according to html tags"
      ],
      "metadata": {
        "id": "sOjk9DZ5pZN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aAGByOw3x81_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** display all the image links from https://www.ibm.com/lk-en.***"
      ],
      "metadata": {
        "id": "w3QZiYJpyuwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "html = urlopen('https://www.ibm.com/lk-en')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "images = bs.find_all('img', {'src' : re.compile('.jpg')}) #jpg to identify the img format\n",
        "for image in images:\n",
        "  print(image['src']+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Th16svQpZQm",
        "outputId": "b614b410-fa73-4410-ac91-47f223d5808f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dam/adobe-cms/default-images/home-consultants.component.crop-16by9-xl.ts=1698848791896.jpg/content/adobe-cms/us/en/homepage/_jcr_content/root/table_of_contents/simple_image\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "html = urlopen('https://www.ibm.com/lk-en')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "images = bs.find_all('img')  #now will give the all the formats images -- jpg,png,, tag = img\n",
        "for image in images:\n",
        "  print(image['src']+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtNQaBBkpZUD",
        "outputId": "e1e9aeb5-ae19-4eb5-b743-d6552f9b8282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dam/adobe-cms/default-images/home-consultants.component.crop-16by9-xl.ts=1698848791896.jpg/content/adobe-cms/us/en/homepage/_jcr_content/root/table_of_contents/simple_image\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_0bGrATxAQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** retrieve all the links in a particular URL.***\n"
      ],
      "metadata": {
        "id": "h6uORVM_ylWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('https://www.sliit.lk')\n",
        "bs =  BeautifulSoup(html, 'lxml')\n",
        "links = bs.find_all('a') #the tag mention in the inspect sourcecode\n",
        "\n",
        "for link in links:\n",
        "  print(link.get('href')) #in a tag href part print\n",
        "\n"
      ],
      "metadata": {
        "id": "ON0fUPjvxAY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('https://www.sliit.lk')\n",
        "bs =  BeautifulSoup(html, 'lxml')\n",
        "links = bs.find_all('a') #the tag mention in the inspect sourcecode\n",
        "\n",
        "# for link in links:\n",
        "#   print(link.get('href')) #in a tag href part print\n",
        "\n",
        "for link in links:\n",
        "  print(link) #print all a tag"
      ],
      "metadata": {
        "id": "Uoe2TMb_xAbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('https://www.sliit.lk')\n",
        "bs =  BeautifulSoup(html, 'lxml')\n",
        "links = bs.find_all('a') #the tag mention in the inspect sourcecode\n",
        "\n",
        "# for link in links:\n",
        "#   print(link.get('href')) #in a tag href part print\n",
        "\n",
        "# for link in links:\n",
        "#   print(link)\n",
        "\n",
        "for link in links:\n",
        "  print(link.text) #print the texts related to each links/get the content insidethe a--> href\n"
      ],
      "metadata": {
        "id": "heTcfgIWxAeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4oef5FbxAhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "display section that contain a specified string in a given web\n",
        "page"
      ],
      "metadata": {
        "id": "KJhI8ZyMIh7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the content related to specific string"
      ],
      "metadata": {
        "id": "E-KBGIv1IgJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re #just import it other wise error in conda environment\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
        "req=requests.get(url)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "str1 = bs.find_all(string=re.compile('Hove')) #Hove War can\n",
        "for txt in str1:\n",
        "  print(txt)"
      ],
      "metadata": {
        "id": "9QEc1N6nIgQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NwPHOSMXIgX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "obtain html code of parent content of a particular html tag."
      ],
      "metadata": {
        "id": "4GTCAzZMSCiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just want to get the all the tags  which have in top of a particular tag"
      ],
      "metadata": {
        "id": "8vFlLwpJIgfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = \"https://www.sliit.lk/\"\n",
        "req=requests.get(url)\n",
        "print(req)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "print(bs.title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_ZbUgbVIgmq",
        "outputId": "6c04d0b4-f360-4663-de46-8551422984af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n",
            "<title>SLIIT | Sri Lanka Institute of Information Technology</title>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#========urllib==== can directly open url    same thing different way\n",
        "(give whole codehtml no need to convert)\n",
        "# from urllib.request import urlopen\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# html = urlopen('https://www.sliit.lk')\n",
        "# bs =  BeautifulSoup(html, 'lxml')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=======requests=====have to call-->req=requests.get(url) this\n",
        "(give the response 200,123 like so have to convert in to html)\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# url = \"https://www.sliit.lk/\"\n",
        "# req=requests.get(url)"
      ],
      "metadata": {
        "id": "CO64OyoIIgw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = \"https://www.sliit.lk/\"\n",
        "req=requests.get(url)\n",
        "#print(req)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "print(bs.title)\n",
        "print(bs.title.text)\n",
        "# print(bs.title.parent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsV4gZbxT8oM",
        "outputId": "efc70206-174f-4a7c-cc8b-6fa957d1cb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title>SLIIT | Sri Lanka Institute of Information Technology</title>\n",
            "SLIIT | Sri Lanka Institute of Information Technology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = \"https://www.sliit.lk/\"\n",
        "req=requests.get(url)\n",
        "#print(req)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "# print(bs.title)\n",
        "# print(bs.title.text)\n",
        "print(bs.title.parent) #give all parent tag the codes above the given title"
      ],
      "metadata": {
        "id": "L6vwT8TZT8qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6B-zoZFT8tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "o obtain html code of child nodes inside a particular html tag"
      ],
      "metadata": {
        "id": "jDAfrb7nV2Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = \"https://www.sliit.lk/\"\n",
        "req=requests.get(url)\n",
        "#print(req)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "# print(bs.header)\n",
        "# print(bs.header.text)\n",
        "print(bs.header.children)  #give it as a list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlU7vEfUT8v1",
        "outputId": "2e482d75-e665-4a6a-86ff-8d5105186c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<list_iterator object at 0x7dd8a39cd5d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(*bs.header.children) #all the tags indide the header"
      ],
      "metadata": {
        "id": "X1Gb7a54T8zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiC0alRdXQhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "uild a web scrapper using BeautifulScraper to extract(scrape) movie data(url, name,\n",
        "summary) from https://www.imdb.com/chart/top/ in to a excel file."
      ],
      "metadata": {
        "id": "EXZdOC_nXXH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Downloading imdb top 250 movie's data\n",
        "url = 'http://www.imdb.com/chart/top'\n",
        "req = requests.get(url)\n",
        "bs= BeautifulSoup(req.text, \"lxml\")\n",
        "\n",
        "movies = bs.select('td.titleColumn')\n",
        "crew = [a.attrs.get('title') for a in bs.select('td.titleColumn a')]\n",
        "ratings = [b.attrs.get('data-value')\n",
        "        for b in bs.select('td.posterColumn span[name=ir]')]\n",
        "\n",
        "# create a empty list for storing\n",
        "# movie information\n",
        "list = []\n",
        "\n",
        "# Iterating over movies to extract\n",
        "# each movie's details\n",
        "for index in range(0, len(movies)):\n",
        "\n",
        "    # Separating movie into: 'place',\n",
        "    # 'title', 'year'\n",
        "    movie_string = movies[index].get_text()\n",
        "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
        "    movie_title = movie[len(str(index))+1:-7]\n",
        "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
        "    place = movie[:len(str(index))-(len(movie))]\n",
        "    data = {\"place\": place,\n",
        "            \"movie_title\": movie_title,\n",
        "            \"rating\": ratings[index],\n",
        "            \"year\": year,\n",
        "            \"star_cast\": crew[index],\n",
        "            }\n",
        "    list.append(data)\n",
        "\n",
        "for movie in list:\n",
        "    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
        "          ') -', 'Starring:', movie['star_cast'], movie['rating'])\n",
        "\n"
      ],
      "metadata": {
        "id": "l0WT0j5LXQkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Downloading imdb top 250 movie's data\n",
        "url = 'http://www.imdb.com/chart/top'\n",
        "req = requests.get(url)\n",
        "bs = BeautifulSoup(req.text, \"lxml\")\n",
        "movies = bs.select('td.titleColumn')\n",
        "crew = [a.attrs.get('title') for a in bs.select('td.titleColumn a')]\n",
        "ratings = [b.attrs.get('data-value')\n",
        "        for b in bs.select('td.posterColumn span[name=ir]')]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a empty list for storing\n",
        "# movie information\n",
        "list = []\n",
        "\n",
        "# Iterating over movies to extract\n",
        "# each movie's details\n",
        "for index in range(0, len(movies)):\n",
        "\n",
        "    # Separating movie into: 'place',\n",
        "    # 'title', 'year'\n",
        "    movie_string = movies[index].get_text()\n",
        "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
        "    movie_title = movie[len(str(index))+1:-7]\n",
        "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
        "    place = movie[:len(str(index))-(len(movie))]\n",
        "    data = {\"place\": place,\n",
        "            \"movie_title\": movie_title,\n",
        "            \"rating\": ratings[index],\n",
        "            \"year\": year,\n",
        "            \"star_cast\": crew[index],\n",
        "            }\n",
        "    list.append(data)\n",
        "\n",
        "# printing movie details with its rating.\n",
        "for movie in list:\n",
        "    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
        "        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n",
        "\n",
        "\n",
        "##.......##\n",
        "df = pd.DataFrame(list)\n",
        "df.to_csv('imdb_top_250_movies.csv',index=False)"
      ],
      "metadata": {
        "id": "hqtBUCGRXQm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NeNSXFulXQpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KK2PNgfAXQsy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}